{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot in python\n",
        "\n",
        "Naan Mudhalvan project to develop a chatbot for websites/app to provide instance customer service"
      ],
      "metadata": {
        "id": "2MoctpJ-uAwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing required modules"
      ],
      "metadata": {
        "id": "-5YJX9kaubMk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJC9IsDJTGAN",
        "outputId": "f6d89ee5-998d-4ec5-cd13-ca2b91446aa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')#Sentence tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "zj-nIcZHTbba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import random"
      ],
      "metadata": {
        "id": "UOqyE26MTc_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Processing NLP"
      ],
      "metadata": {
        "id": "Gl0h6uJOuet8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']\n",
        "data_file = open('intents.json').read() # read json file\n",
        "intents = json.loads(data_file) # load json file"
      ],
      "metadata": {
        "id": "W9TcPXutTehG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        #tokenize each word\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)# add each elements into list\n",
        "        #combination between patterns and intents\n",
        "        documents.append((w, intent['tag']))#add single element into end of list\n",
        "        # add to tag in our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ],
      "metadata": {
        "id": "Fjo6icjCThCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet') #lexical database for the English language"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQKNARzuTiNB",
        "outputId": "072ec124-1149-446a-cab1-76448ddaa0cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LgmZ888TjSq",
        "outputId": "7f543af4-fcad-4d13-dcee-fb7f5fe73bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "# documents = combination between patterns and intents\n",
        "print (len(documents), \"documents\\n\", documents, \"\\n\")\n",
        "# classes = intents[tag]\n",
        "print (len(classes), \"classes\\n\", classes, \"\\n\")\n",
        "# words = all words, vocabulary\n",
        "print (len(words), \"unique lemmatized words\\n\", words, \"\\n\")\n",
        "pickle.dump(words,open('words.pkl','wb'))\n",
        "pickle.dump(classes,open('classes.pkl','wb'))"
      ],
      "metadata": {
        "id": "3hSGBF3QTkRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create our training data\n",
        "training = []\n",
        "# create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words\n",
        "    pattern_words = doc[0]\n",
        "    # convert pattern_words in lower case\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "    # create bag of words array,if word match found in current pattern then put 1 otherwise 0.[row * colm(263)]\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # in output array 0 value for each tag ang 1 value for matched tag.[row * colm(8)]\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "# shuffle training and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "# create train and test. X - patterns(words), Y - intents(tags)\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "print(\"Training data created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRC0Bxc2Tn4U",
        "outputId": "ec8c0fc0-9e94-48e9-ec2d-c02feb1d16cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Creation"
      ],
      "metadata": {
        "id": "O9e_t5tHusm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()"
      ],
      "metadata": {
        "id": "PC8lXZwMTsHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "print(\"First layer:\",model.layers[0].get_weights()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-pIfSPTTtb5",
        "outputId": "7841ef3a-dc62-4478-9db8-910c3d448cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First layer: [[ 0.08650128 -0.0634183  -0.07228476 ...  0.0830598   0.06828941\n",
            "   0.0835707 ]\n",
            " [-0.03220329 -0.07752044  0.00102633 ...  0.02529063 -0.04513907\n",
            "  -0.06462342]\n",
            " [-0.01536799  0.10085208  0.09434902 ...  0.0604412   0.02147218\n",
            "   0.09148347]\n",
            " ...\n",
            " [ 0.10917494 -0.02004863 -0.05833514 ... -0.03475115 -0.06402607\n",
            "   0.01667366]\n",
            " [-0.11102267 -0.09436521  0.03962915 ...  0.12295949  0.02094749\n",
            "   0.00144091]\n",
            " [-0.05854262  0.11776628 -0.02101282 ...  0.04151178  0.11692443\n",
            "  -0.08398442]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "iX98E-obTuoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Model"
      ],
      "metadata": {
        "id": "CdH5Hb9CuwL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save('chatbot_model.h5', hist)\n",
        "\n",
        "print(\"model created\")"
      ],
      "metadata": {
        "id": "l6m03TOYTxVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing to test our model"
      ],
      "metadata": {
        "id": "WmsX3fPvu1xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up_sentence(sentence):\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  #print(sentence_words)\n",
        "  # stem each word - create short form for word\n",
        "\n",
        "  sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "  #print(sentence_words)\n",
        "\n",
        "  return sentence_words"
      ],
      "metadata": {
        "id": "pqasZK9RTzD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bow(sentence, words, show_details=True):\n",
        "  # tokenize the pattern\n",
        "\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "  #print(sentence_words)\n",
        "\n",
        "  # bag of words - matrix of N words, vocabulary matrix\n",
        "\n",
        "  bag = [0]*len(words)\n",
        "  #print(bag)\n",
        "\n",
        "  for s in sentence_words:\n",
        "      for i,w in enumerate(words):\n",
        "          if w == s:\n",
        "              # assign 1 if current word is in the vocabulary position\n",
        "              bag[i] = 1\n",
        "              if show_details:\n",
        "                  print (\"found in bag: %s\" % w)\n",
        "              #print (\"found in bag: %s\" % w)\n",
        "  #print(bag)\n",
        "  return(np.array(bag))"
      ],
      "metadata": {
        "id": "i8jZpqTtUM4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_class(sentence, model):\n",
        "  # filter out predictions below a threshold\n",
        "\n",
        "  p = bow(sentence, words,show_details=False)\n",
        "  #print(p)\n",
        "\n",
        "  res = model.predict(np.array([p]))[0]\n",
        "  #print(res)\n",
        "\n",
        "  ERROR_THRESHOLD = 0.25\n",
        "\n",
        "  results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "  #print(results)\n",
        "  # sort by strength of probability\n",
        "\n",
        "  results.sort(key=lambda x: x[1], reverse=True)\n",
        "  #print(results)\n",
        "\n",
        "  return_list = []\n",
        "\n",
        "  for r in results:\n",
        "      return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "\n",
        "  return return_list\n",
        "  #print(return_list)"
      ],
      "metadata": {
        "id": "pXp1Bx-QUQzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getResponse(ints, intents_json):\n",
        "  tag = ints[0]['intent']\n",
        "  #print(tag)\n",
        "\n",
        "  list_of_intents = intents_json['intents']\n",
        "  #print(list_of_intents)\n",
        "\n",
        "  for i in list_of_intents:\n",
        "      if(i['tag']== tag):\n",
        "          result = random.choice(i['responses'])\n",
        "          break\n",
        "  return result"
      ],
      "metadata": {
        "id": "5KiiSHJrUXGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_response(text):\n",
        "  ints = predict_class(text, model)\n",
        "  res = getResponse(ints, intents)\n",
        "  #print(res)\n",
        "  return res\n"
      ],
      "metadata": {
        "id": "C0CmnLl1UeqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing model"
      ],
      "metadata": {
        "id": "texmERiou4m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = True\n",
        "while start:\n",
        "  query = input('Enter Message:')\n",
        "  if query in ['quit','exit','bye']:\n",
        "      start = False\n",
        "      continue\n",
        "  try:\n",
        "      res = chatbot_response(query)\n",
        "      print(res)\n",
        "  except:\n",
        "      print('You may need to rephrase your question.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "id": "pqv5oHtoUiZy",
        "outputId": "4cef7ed8-4832-4165-fc36-5a8e565ffee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Message:Who are you?\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "I am your chatbot assist.\n",
            "Enter Message:what can you do?\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "I can answer to low-intermediate questions regarding college\n",
            "Enter Message:Where is your college?\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "VV College of Engineering, V.V.Nagar, Tisaiyanvilai(Via), Tirunelveli(Dist.)\n",
            "Enter Message:Who is Principal?\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Dr. K. S. Saji  is college principal and if you need any help then call your branch hod first.That is more appropriate\n",
            "Enter Message:Who is computer science HOD?\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Muthulakshmi\n",
            "Enter Message:When college opens?\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "College is open 9:30 AM - 4:30 PM Monday-Friday!\n",
            "Enter Message:What the hell!!!!!\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "please use appropriate language\n",
            "Enter Message:Who created you?\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Kim Poobi\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-fbb2bff7ef41>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Enter Message:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'quit'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'exit'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'bye'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "0FRT31feUoC7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}